{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "88f82638645f62282e67bf8fe72ecfb9e31ad2b55dd59a6a97e26960dc584bdd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ds_path = os.path.join(os.curdir, 'codes')\n",
    "sys.path.append(ds_path)\n",
    "from dataset.mnist import load_mnist\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(784, 100)\n(100,)\n(100, 10)\n(10,)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, \n",
    "    input_size, hidden_size, output_size,\n",
    "    weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] =  numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] =  numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.10192721 0.09820728 0.10370459 0.09850884 0.10026217 0.10763921\n  0.09002864 0.09952788 0.10177139 0.09842279]\n [0.10181376 0.09827803 0.10376801 0.09841572 0.10009225 0.10791004\n  0.0902191  0.09945117 0.10145255 0.09859936]\n [0.10204341 0.09838566 0.10323803 0.09873    0.10027095 0.10746568\n  0.09004928 0.09944255 0.10155758 0.09881685]\n [0.1021387  0.0983437  0.10334654 0.09883771 0.10016845 0.10727726\n  0.09042861 0.09948092 0.1013844  0.09859372]\n [0.10188162 0.09791774 0.10287504 0.09908849 0.10049242 0.10778224\n  0.09035893 0.09959704 0.10132128 0.09868519]\n [0.10214211 0.09796312 0.10319249 0.09873114 0.09989047 0.10751522\n  0.09035651 0.09941253 0.10165652 0.09913989]\n [0.10212832 0.09839539 0.10278573 0.09869288 0.10026253 0.10773775\n  0.09012472 0.09968007 0.10133891 0.0988537 ]\n [0.10187036 0.09820348 0.10359513 0.0987349  0.10006096 0.10775793\n  0.09009742 0.09927943 0.1017182  0.09868219]\n [0.10204284 0.09845589 0.10332827 0.09858241 0.10036918 0.10770021\n  0.09034292 0.09926058 0.10123085 0.09868685]\n [0.10231555 0.09798306 0.10352321 0.09875815 0.10042711 0.10746569\n  0.09003877 0.09915378 0.10143456 0.09890012]\n [0.10181243 0.09832867 0.10322136 0.09897354 0.10000345 0.10753214\n  0.09007135 0.09954068 0.10155377 0.0989626 ]\n [0.10203747 0.09815108 0.10350523 0.09872535 0.10000831 0.10790079\n  0.09040799 0.09926768 0.10161772 0.09837839]\n [0.10200889 0.09809634 0.10346978 0.0986101  0.10014658 0.10759633\n  0.09013763 0.09965606 0.10140813 0.09887017]\n [0.1018966  0.09808514 0.10302736 0.09870019 0.10071194 0.10797886\n  0.09002916 0.09953717 0.10141702 0.09861655]\n [0.10180843 0.09838766 0.1033758  0.09858314 0.10027475 0.1079692\n  0.08997101 0.09949016 0.10167447 0.09846537]\n [0.10204116 0.09822693 0.10308142 0.09877705 0.10027959 0.10765766\n  0.0899964  0.09952559 0.10177572 0.09863848]\n [0.10166239 0.09809826 0.10340201 0.09906217 0.10012067 0.10783719\n  0.09022582 0.09930972 0.10172484 0.09855694]\n [0.10204883 0.09849712 0.10305767 0.09864475 0.10022248 0.10786247\n  0.0899664  0.09968025 0.10161367 0.09840636]\n [0.10203527 0.09825082 0.10310124 0.09877931 0.10012502 0.10796573\n  0.09013998 0.09968979 0.10130949 0.09860335]\n [0.10183016 0.09826586 0.10306963 0.09877075 0.10006    0.10776553\n  0.08988205 0.09974378 0.10184555 0.09876669]\n [0.10209416 0.09799019 0.1030597  0.09891468 0.10006047 0.10773844\n  0.09001014 0.09969755 0.10172722 0.09870744]\n [0.10167336 0.09856694 0.103563   0.09891549 0.10019957 0.10778563\n  0.09012089 0.09921618 0.10164482 0.09831413]\n [0.10204388 0.09837796 0.10314184 0.0988078  0.10040306 0.10750709\n  0.08984978 0.0996725  0.10157774 0.09861834]\n [0.10219249 0.09893493 0.10324306 0.09862297 0.10011385 0.10748621\n  0.09048163 0.09929712 0.10120938 0.09841836]\n [0.10166299 0.09824867 0.10347377 0.09892398 0.10010537 0.10806587\n  0.09023074 0.0992962  0.10150897 0.09848344]\n [0.10172994 0.09852108 0.10327853 0.09887109 0.10020983 0.10791717\n  0.09019718 0.09942842 0.10137157 0.0984752 ]\n [0.10196614 0.09834245 0.10344713 0.09856502 0.10042913 0.1074956\n  0.09020835 0.09954262 0.10124569 0.09875786]\n [0.10175583 0.09839307 0.10328954 0.09874566 0.10034609 0.10759369\n  0.09030316 0.09929542 0.1013939  0.09888364]\n [0.1020538  0.09835068 0.10316742 0.0989453  0.10035422 0.10752664\n  0.08998163 0.09926128 0.10160083 0.09875821]\n [0.10212823 0.09868853 0.10298109 0.09877302 0.09987699 0.10766449\n  0.09031228 0.09961778 0.10101867 0.09893893]\n [0.10160105 0.09851243 0.10338589 0.09905064 0.10010634 0.10752927\n  0.09020094 0.09934883 0.10139802 0.09886657]\n [0.10182525 0.09842098 0.103503   0.09873863 0.10010319 0.10753363\n  0.09008594 0.0995246  0.10140985 0.09885492]\n [0.10198385 0.09821856 0.10359542 0.09875983 0.10000554 0.1074778\n  0.08990865 0.09956429 0.10163086 0.09885519]\n [0.10187171 0.09808642 0.103279   0.09885694 0.10034504 0.10769193\n  0.09012278 0.09947793 0.10170586 0.09856239]\n [0.10182988 0.09813361 0.10335469 0.09887987 0.10029705 0.10753408\n  0.09040139 0.09959635 0.10161639 0.0983567 ]\n [0.10200466 0.09838235 0.10305494 0.09870468 0.10034712 0.10821122\n  0.08997286 0.09949672 0.10122928 0.09859616]\n [0.10178912 0.09836301 0.10307207 0.09853996 0.10045361 0.1077311\n  0.08993132 0.09994626 0.10148214 0.09869142]\n [0.10172036 0.09863534 0.10306661 0.09910962 0.10046777 0.10788421\n  0.09019016 0.09918257 0.10145171 0.09829166]\n [0.10186566 0.09848215 0.10367802 0.09884221 0.10004706 0.10771781\n  0.09039276 0.09923809 0.10120593 0.0985303 ]\n [0.10199473 0.09841036 0.10316532 0.09881908 0.10018006 0.10784104\n  0.09019649 0.09911756 0.10166265 0.09861272]\n [0.10206747 0.09847006 0.10309496 0.09863926 0.10015752 0.10784812\n  0.08999716 0.09964717 0.1014745  0.09860378]\n [0.10194673 0.09838283 0.10315917 0.09855609 0.10018777 0.1076668\n  0.09013245 0.09984365 0.10153876 0.09858575]\n [0.1019405  0.09826835 0.1033117  0.09885944 0.10000334 0.10762317\n  0.09021814 0.09926356 0.10160489 0.09890691]\n [0.10182292 0.09834063 0.10354204 0.09854807 0.10014264 0.10782179\n  0.08989534 0.09950783 0.10181688 0.09856186]\n [0.1021093  0.09835339 0.10303439 0.09895015 0.10009704 0.10790069\n  0.09014948 0.09939768 0.10145207 0.09855582]\n [0.10188076 0.09803571 0.10342735 0.09870023 0.10023434 0.10762673\n  0.09010071 0.09965962 0.10157314 0.09876141]\n [0.10208856 0.09816159 0.10320656 0.09875502 0.10012977 0.10742994\n  0.09012805 0.09970193 0.10161469 0.09878389]\n [0.10217029 0.09805013 0.10305718 0.09904344 0.10012567 0.10747622\n  0.09004954 0.09950207 0.10158892 0.09893655]\n [0.10207965 0.09809506 0.1033579  0.09894961 0.10024069 0.10748826\n  0.09007032 0.09937022 0.10178122 0.09856707]\n [0.10187462 0.09835943 0.10350174 0.09861522 0.10005066 0.10760486\n  0.09004755 0.09976306 0.10155423 0.09862864]\n [0.10175377 0.09834494 0.10315088 0.09856807 0.10021424 0.10769212\n  0.09034725 0.09980922 0.10142844 0.09869107]\n [0.10192255 0.09846563 0.10284673 0.09889982 0.10023606 0.10781037\n  0.09007495 0.0997196  0.1014561  0.0985682 ]\n [0.10199295 0.09810061 0.10320527 0.09886192 0.10051364 0.10743844\n  0.0899739  0.09962194 0.1017436  0.09854773]\n [0.10203833 0.09818478 0.1031645  0.0987601  0.10025562 0.10768962\n  0.09046125 0.09947306 0.10116865 0.09880409]\n [0.1019592  0.09804646 0.1031125  0.09880145 0.10002352 0.1077689\n  0.09013386 0.09980257 0.10139797 0.09895356]\n [0.10203955 0.09805858 0.10359471 0.09873931 0.10010111 0.10766306\n  0.09034728 0.09949112 0.10136816 0.09859711]\n [0.10202638 0.09834769 0.10321552 0.0989816  0.10025699 0.10744156\n  0.0902722  0.09941053 0.10132626 0.09872127]\n [0.10163686 0.09809593 0.10325416 0.09878993 0.1000467  0.10829413\n  0.09013683 0.09968843 0.10134884 0.09870821]\n [0.10238332 0.09825955 0.10341667 0.0987284  0.10013852 0.1076606\n  0.09020286 0.0992043  0.10131145 0.09869433]\n [0.10188411 0.0981665  0.1037713  0.09868999 0.09985908 0.10754831\n  0.090464   0.09930255 0.10159834 0.09871581]\n [0.10157951 0.09798154 0.10328073 0.0990218  0.10057454 0.10766505\n  0.09019216 0.09946887 0.10159639 0.0986394 ]\n [0.10186574 0.09822689 0.10299806 0.09879607 0.10023078 0.1079456\n  0.09001188 0.09973098 0.10186184 0.09833214]\n [0.10231401 0.09836954 0.10344539 0.09849733 0.09994804 0.10779696\n  0.08999951 0.09916014 0.101735   0.09873409]\n [0.10185792 0.09825786 0.10350079 0.09889511 0.09988612 0.10782463\n  0.09003461 0.09956652 0.10165241 0.09852403]\n [0.10172037 0.0982118  0.10344828 0.09857131 0.10025281 0.10763864\n  0.09008584 0.09964348 0.1017465  0.09868095]\n [0.10200734 0.09813802 0.1033708  0.09887494 0.10009011 0.10761647\n  0.09026572 0.09959882 0.10127757 0.09876023]\n [0.10240993 0.0982853  0.10288105 0.09895474 0.10010024 0.10739932\n  0.09042854 0.09949338 0.10153795 0.09850956]\n [0.10201366 0.09842219 0.10322677 0.09884541 0.09963525 0.1078299\n  0.09034511 0.09953467 0.10139572 0.09875133]\n [0.10177227 0.09797776 0.10295794 0.09907933 0.10050868 0.10772008\n  0.09019893 0.09935863 0.10174527 0.09868111]\n [0.10196037 0.0981862  0.10371425 0.09872917 0.10018842 0.10763582\n  0.09001246 0.09947643 0.10147273 0.09862415]\n [0.10184506 0.09856492 0.1032646  0.0987498  0.099832   0.10767342\n  0.09013533 0.09952724 0.10176156 0.09864608]\n [0.10164442 0.0982079  0.10332484 0.09868869 0.10020624 0.107548\n  0.08993283 0.09988157 0.10173644 0.09882907]\n [0.10206997 0.09840917 0.10344301 0.09875117 0.09989034 0.10781407\n  0.09014593 0.09949185 0.10131325 0.09867124]\n [0.10203628 0.09829528 0.1032399  0.09898884 0.10019365 0.10761113\n  0.09014657 0.09948965 0.10146531 0.0985334 ]\n [0.10185924 0.09812295 0.10353822 0.09901055 0.10040942 0.10761039\n  0.09027787 0.09918215 0.10118082 0.0988084 ]\n [0.10175329 0.09856365 0.10319142 0.09863766 0.09993039 0.10780858\n  0.08992501 0.09978299 0.10169731 0.0987097 ]\n [0.10177124 0.09841081 0.10356453 0.09866893 0.09992954 0.10795572\n  0.08990967 0.09946186 0.10167154 0.09865615]\n [0.10150279 0.09797898 0.1033168  0.09883401 0.10050594 0.10793624\n  0.09036004 0.09934688 0.10153627 0.09868204]\n [0.10204258 0.09761427 0.10335898 0.09898576 0.10027552 0.10774627\n  0.08993108 0.09963326 0.10170556 0.09870671]\n [0.10186638 0.09826693 0.10347339 0.09871107 0.1001139  0.10766118\n  0.09011017 0.09951138 0.10176063 0.09852496]\n [0.10188939 0.09856152 0.10312982 0.09887805 0.10026082 0.10739328\n  0.09007856 0.09945529 0.10160117 0.09875211]\n [0.101934   0.09841771 0.10291111 0.09891478 0.10010739 0.10742703\n  0.09017875 0.09974701 0.10169884 0.09866338]\n [0.10210526 0.09845256 0.10335637 0.09860603 0.09974966 0.10774623\n  0.08996685 0.09960985 0.1017313  0.0986759 ]\n [0.10189306 0.09813354 0.10340913 0.09898885 0.1003146  0.1075967\n  0.08988949 0.09943752 0.10154081 0.09879631]\n [0.10153296 0.09824554 0.10362992 0.09866714 0.10034639 0.1077917\n  0.09006591 0.09943215 0.10191283 0.09837545]\n [0.10204584 0.09828515 0.10344968 0.09841341 0.09996356 0.10754305\n  0.09038036 0.09948919 0.10127282 0.09915695]\n [0.10146998 0.0981267  0.10345839 0.09859749 0.10017365 0.10800321\n  0.09015162 0.09972468 0.10150472 0.09878954]\n [0.10178167 0.09840481 0.10323457 0.09898776 0.10033319 0.10795325\n  0.08992073 0.09948281 0.10119302 0.09870818]\n [0.10205581 0.09817104 0.10339963 0.09851795 0.10010711 0.10794908\n  0.08985925 0.09972225 0.10143058 0.09878731]\n [0.10199766 0.09802606 0.10322734 0.09892293 0.10009511 0.10735623\n  0.09026172 0.09965175 0.10179903 0.09866216]\n [0.10205286 0.09861023 0.10325502 0.09856185 0.1002298  0.10740851\n  0.09004328 0.09985553 0.1010559  0.09892702]\n [0.10188695 0.09792831 0.10305062 0.09917385 0.10036314 0.1076833\n  0.09041651 0.09951256 0.10152968 0.09845508]\n [0.10171836 0.09815203 0.10339491 0.09917333 0.10033732 0.10749198\n  0.08995789 0.0994823  0.1014747  0.09881719]\n [0.10233941 0.0981416  0.10315923 0.09889469 0.10015982 0.10736164\n  0.09011931 0.09958096 0.10142959 0.09881375]\n [0.10182265 0.09805554 0.10340002 0.0987749  0.10000464 0.10773823\n  0.09014793 0.09958428 0.10177152 0.09870029]\n [0.10215415 0.09811917 0.10314601 0.09843618 0.10051663 0.10775167\n  0.09022473 0.09975263 0.10162219 0.09827664]\n [0.10205133 0.09827529 0.10349005 0.09888653 0.10013792 0.10753605\n  0.09006048 0.09943811 0.10157882 0.09854542]\n [0.10189117 0.0985377  0.10296729 0.09878225 0.09997445 0.10804961\n  0.09027712 0.09968795 0.10116758 0.09866489]\n [0.10224689 0.09820984 0.10303528 0.09878978 0.10005885 0.10748612\n  0.09028453 0.09955321 0.10170679 0.09862872]\n [0.10206401 0.09835587 0.10316199 0.09880744 0.10022327 0.10768503\n  0.08990239 0.09941353 0.10171138 0.09867508]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # 伪输入数据（100笔）\n",
    "y = net.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(784, 100)\n(100,)\n(100, 10)\n(10,)\n"
     ]
    }
   ],
   "source": [
    "t = np.random.rand(100, 10) # 伪正确解标签（100笔）\n",
    "grads = net.gradient(x, t) # 计算梯度\n",
    "print(grads['W1'].shape) # (784, 100)\n",
    "print(grads['b1'].shape) # (100,)\n",
    "print(grads['W2'].shape) # (100, 10)\n",
    "print(grads['b2'].shape) # (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train acc, test acc | 0.11395, 0.115\n",
      "train acc, test acc | 0.7944833333333333, 0.8013\n",
      "train acc, test acc | 0.87805, 0.8836\n",
      "train acc, test acc | 0.8984166666666666, 0.9023\n",
      "train acc, test acc | 0.9069166666666667, 0.9105\n",
      "train acc, test acc | 0.9140333333333334, 0.9168\n",
      "train acc, test acc | 0.9184666666666667, 0.9204\n",
      "train acc, test acc | 0.9228333333333333, 0.926\n",
      "train acc, test acc | 0.9272833333333333, 0.9294\n",
      "train acc, test acc | 0.9312833333333334, 0.9331\n",
      "train acc, test acc | 0.9346, 0.9359\n",
      "train acc, test acc | 0.93745, 0.9389\n",
      "train acc, test acc | 0.9399166666666666, 0.9411\n",
      "train acc, test acc | 0.9420833333333334, 0.9428\n",
      "train acc, test acc | 0.94465, 0.9455\n",
      "train acc, test acc | 0.9463166666666667, 0.9452\n",
      "train acc, test acc | 0.9485, 0.9464\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "# 平均每个epoch的重复次数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    if(i % iter_per_epoch == 0):\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"train acc, test acc | {train_acc}, {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n(28, 28)\n3\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-11-30T21:43:00.350578</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p974d6fcd67)\">\n    <image height=\"218\" id=\"image8c352cefa7\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAE0klEQVR4nO3dsWsUaRzH4XcPqxS2QhoJ2Uar64yIWwWuFgLpbE+4Ko1/hI2VEFs7q61tN4ixS6WNQWwCtha2XnXFofu+6mS+k5l9nvY1O4Pw4YX58c7M9mcH3wrQqz+GvgHYBEKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CLg29A1soq8P7lTXdx6/r66/uLmqru++fPTL9/Sf7VX97YNby7e//dubzI4GAUKDAKFBgNAgQGgQIDQIEBoEzHy2qR833lxfu9aag11lDz8tquuf734J3cm42NEgQGgQIDQIEBoECA0ChAYBQoMAc7Q1+j4zNlXmbD9mR4MAoUGA0CBAaBAgNAgQGgQIDQI2do7WmpOdPHve27Vbs6bXp7d7u3Yppdzbe7d2re/53/1//l67NuV3RtrRIEBoECA0CBAaBAgNAoQGAUKDgI2do726OOv192vfKJsfnfZ67S76/n+pqc3YShn3nM2OBgFCgwChQYDQIEBoECA0CLg29A305cPTvca/OKuuto6yfHxyq7o+X17dR/g1rUfsfR4fuljMquvzZW+X7p0dDQKEBgFCgwChQYDQIEBoECA0CJjsHK2r1ivfxjona2kdRdldrD/+U0op54fHl3k7k2FHgwChQYDQIEBoECA0CBAaBAgNAiY7R9teNd6id9jx7+EX2NEgQGgQIDQIEBoECA0ChAYBQoOAyc7RWueq7pfpfiKoT/f23g19C6NkR4MAoUGA0CBAaBAgNAgQGgRM9vF+i8f3P/bq4mywa0/5aJIdDQKEBgFCgwChQYDQIEBoECA0CNjYOdqU3Xhzvbr+4uYqdCff2325/rNPU/0UVil2NIgQGgQIDQKEBgFCgwChQYDQIMAcbY0PT/eq663Xrg05qxpSbU5WSinzo+nOymrsaBAgNAgQGgQIDQKEBgFCgwChQYA52hrnh8dD38IgHn5aVNc/PrlVXZ/ymbIu7GgQIDQIEBoECA0ChAYBQoMAoUGAOdoarXnSpp434/fY0SBAaBAgNAgQGgQIDQKEBgGz/dnBt6Fvgv/7+uBOdf3k2fPQnXyv6zGareXby7yd0bCjQYDQIEBoECA0CBAaBAgNAoQGAeZoI9Sas+08fl9d7/OIT2vO9vnul96ufZXZ0SBAaBAgNAgQGgQIDQKEBgFCg4BRv26uNk9qzZJaXp/e7vT3NdurbqPLi8Wsur7T6de7ac3o/ip/Zm7kirGjQYDQIEBoECA0CBAaBAgNAoQGAaOeo9XmSSddz1z1+Vmmw/5+emit82ilOI8G9ERoECA0CBAaBAgNAoQGAUKDgFHP0eZHp2vXdsuj6t+eHx5f9u1QfuL7aMX30YCeCA0ChAYBQoMAoUGA0CBg1I/3a2qP/ktpP/7vasjxQeuoSpdX6bVelbe13MzH9y12NAgQGgQIDQKEBgFCgwChQYDQIGC2Pzvo9g0hoMmOBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQI+BdyPKQR4SboOQAAAABJRU5ErkJggg==\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m99faae7c25\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m99faae7c25\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m99faae7c25\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m99faae7c25\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m99faae7c25\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m99faae7c25\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m99faae7c25\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma1ff7734c4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma1ff7734c4\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma1ff7734c4\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma1ff7734c4\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma1ff7734c4\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma1ff7734c4\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma1ff7734c4\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p974d6fcd67\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# random test\n",
    "random_idx = np.random.randint(x_test.shape[0])\n",
    "rx = x_test[random_idx] * 255\n",
    "print(np.argmax(t_test[random_idx]))\n",
    "rx = rx.reshape(28,28)\n",
    "print(rx.shape)\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "imshow(Image.fromarray(np.uint8(rx)))\n",
    "\n",
    "p = network.predict(x_test[random_idx])\n",
    "print(np.argmax(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}